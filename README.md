# BioVisLLM: Visualizing Spatial Transcriptomics with LLM Analysis

This project provides an interactive platform for visualizing spatial transcriptomics data and leveraging Large Language Models (LLMs) like BioBERT and Google Gemini to analyze relationships between selected cell types and genes across different biological contexts.

## Overview

BioVisLLM integrates spatial data visualization using Deck.gl with dynamic analysis components powered by LLMs. Users can select tissue samples, interactively choose cell types and genes of interest, and receive contextual biological insights generated by either a locally run BioBERT model or the cloud-based Google Gemini API. The goal is to bridge the gap between complex spatial data exploration and automated biological interpretation.

## Features

*   **Multi-Sample Visualization:** Load and view multiple spatial transcriptomics samples side-by-side.
*   **Interactive Selection:** Select specific cell types and genes directly from the visualization interface.
*   **Component-Based Analysis:** Explore biological questions through five dedicated analysis components:
    1.  Regional Gene-Cell Relationship
    2.  Spatial Comparison Between Regions
    3.  Pathway and Functional Enrichment
    4.  Gene Co-expression and Interaction
    5.  Disease or Immune Relevance
*   **LLM Integration:** Choose between different LLMs for analysis:
    *   **BioBERT:** A locally run model specialized in biomedical text (primarily Question Answering).
    *   **Gemini:** A powerful generative model accessed via Google AI API (requires API key).
*   **Model Switching:** Easily switch between BioBERT and Gemini to compare analysis results.
*   **Dynamic Analysis:** Analyses update automatically based on user selections of cells and genes.

## Demo & Screenshots

*(This section is intentionally left blank for you to add images and videos - see guide below)*

*Replace the placeholders below with your actual files located in the `assets` folder.*

**Application Screenshot:**

![Application Screenshot](assets/screenshot.png)

**Short Demo (GIF):**

![Demo GIF](assets/demo.gif)

*(Alternatively, if you use a short MP4 video):*
<!-- ![Demo Video](assets/demo.mp4) -->

## Technology Stack

*   **Frontend:** React, Ant Design (AntD), Deck.gl
*   **Backend:** Python, Flask, Pandas, Scanpy
*   **LLM Integration:**
    *   Hugging Face Transformers (for local BioBERT)
    *   `google-generativeai` SDK (for Gemini API)
*   **Data Handling:** AnnData, Zarr (implied by backend dependencies)

## Setup and Installation

Follow these steps to set up the project locally.

### Prerequisites

*   Node.js and npm (or Yarn) for the frontend.
*   Python (>= 3.8 recommended) and pip for the backend.
*   Git for cloning the repository.
*   **(Optional but Recommended for Gemini)** A Google Gemini API Key.

### Backend Setup

1.  **Clone the repository:**
    ```bash
    git clone <your-repository-url>
    cd BioVisLLM/backend
    ```

2.  **Create and activate a virtual environment:**
    ```bash
    # Windows
    python -m venv venv
    .\venv\Scripts\activate

    # macOS/Linux
    python3 -m venv venv
    source venv/bin/activate
    ```

3.  **Install Python dependencies:**
    *(Note: requirements.txt contains many libraries. Installation might take some time and may require build tools.)*
    ```bash
    pip install -r requirements.txt
    ```
    *(If you encounter errors during installation, you might need specific system libraries or adjust versions.)*

4.  **Download BioBERT Model (if not already present):**
    *   The `biobert_service.py` currently expects the model files to be in `backend/models/biobert`. Ensure the BioBERT model files (like `pytorch_model.bin`, `config.json`, `vocab.txt`) are downloaded and placed in this directory. You can typically download these from the Hugging Face Hub page for the specific BioBERT model you intend to use (e.g., `dmis-lab/biobert-base-cased-v1.1-squad`).

5.  **Set Gemini API Key (Required for Gemini):**
    *   Obtain a free API key from [Google AI Studio](https://aistudio.google.com/app/apikey).
    *   Set the key as an environment variable **before** running the server. Replace `YOUR_API_KEY_HERE` with your actual key:
        ```bash
        # Windows PowerShell
        $env:GEMINI_API_KEY="YOUR_API_KEY_HERE"

        # Windows Command Prompt
        set GEMINI_API_KEY=YOUR_API_KEY_HERE

        # macOS/Linux
        export GEMINI_API_KEY="YOUR_API_KEY_HERE"
        ```

6.  **Run the Backend Server:**
    ```bash
    python server.py
    ```
    The server should start, typically on `http://localhost:5003`. Note the port number.

### Frontend Setup

1.  **Navigate to the frontend directory:**
    ```bash
    cd ../frontend 
    ```

2.  **Install Node.js dependencies:**
    ```bash
    npm install
    # or if you use yarn:
    # yarn install
    ```

3.  **Run the Frontend Development Server:**
    ```bash
    npm start
    # or if you use yarn:
    # yarn start
    ```
    This will usually open the application automatically in your web browser at `http://localhost:3000`.

## Usage

1.  Ensure both the backend and frontend servers are running.
2.  Open `http://localhost:3000` in your web browser.
3.  Select one or more samples from the sample selection dropdown.
4.  Click "Confirm" to load the data.
5.  Use the "Model Selector" (top-right) to choose between "BioBERT" and "Gemini".
6.  Interact with the `MultiSampleViewer`:
    *   Select/deselect cell types in the sample panel on the left.
    *   Select/deselect genes in the sample panel on the left (switch the radio button to "Genes").
7.  Observe the five analysis components update based on your selections and the chosen model.

## Model Explanation

This application allows comparing insights from two different types of LLMs:

*   **BioBERT:**
    *   **Type:** Extractive Question Answering (QA) model, specifically trained on biomedical text.
    *   **How it works here:** It tries to *find* the answer to the specific component question *within* the limited context provided (which includes the names of selected cells and genes).
    *   **Limitations:** Because it's *extractive*, it cannot *generate* new analytical paragraphs or summaries. If the exact answer isn't present in the simple context we provide, it often returns irrelevant snippets or the `[BioBERT QA could not find...]` message. It runs locally, requiring model download but no API calls.
*   **Gemini (via API):**
    *   **Type:** Generative Large Language Model.
    *   **How it works here:** We send the specific component question (which includes the selected cell/gene names) directly to the Gemini API. The model *understands* the question and *generates* a relevant textual analysis based on its vast training data.
    *   **Advantages:** Capable of producing detailed, coherent analytical paragraphs tailored to the input question, cells, and genes.
    *   **Requirements:** Requires an internet connection and a valid `GEMINI_API_KEY` configured as an environment variable. Usage is subject to Google's free tier limits.

## Adding Images and Videos to README

You can easily add visuals to this README file using Markdown.

1.  **Create an Assets Folder:** It's good practice to create a folder (e.g., `docs/images` or simply `assets` at the root of your project) to store your images and videos.

2.  **Adding Images:**
    *   Place your screenshot (e.g., `screenshot.png`) in the assets folder.
    *   Use the following Markdown syntax in the `README.md` file:
        ```markdown
        ![Alt text describing the image](assets/screenshot.png)
        ```
        *Example:*
        ![Application Screenshot](assets/screenshot.png)

3.  **Adding Videos:**
    *   **Short Clips (GIFs):** Animated GIFs are excellent for short demos and work like images:
        ```markdown
        ![Demo GIF](assets/demo.gif)
        ```
    *   **Longer Videos (MP4, MOV):** GitHub supports embedding MP4 and MOV files directly in Markdown (up to a certain size limit, often 10MB, check GitHub's current limits).
        *   Place your video (e.g., `demo.mp4`) in the assets folder.
        *   Use the same syntax as images:
            ```markdown
            ![Demo Video](assets/demo.mp4)
            ```
            GitHub should render this as a video player.

## Configuration

*   **Backend Port:** The backend server runs on port 5003 by default (see `server.py`).
*   **Frontend Port:** The frontend runs on port 3000 by default (standard for Create React App).
*   **Proxy:** The frontend uses a proxy (configured in `frontend/package.json` or `setupProxy.js` if it exists) to forward API requests from `localhost:3000` to the backend at `localhost:5003`.
*   **Gemini API Key:** Must be set as the `GEMINI_API_KEY` environment variable for the backend process.

## License

No Licence
This project is currently private/public. 